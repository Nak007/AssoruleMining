{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, sys\n",
    "from sklearn.ensemble import (RandomForestClassifier, \n",
    "                              ExtraTreesClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from AssoruleMining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(train, test):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    x = np.arange(len(train)) + 1 \n",
    "    \n",
    "    # Common plotting style\n",
    "    base_style = dict(lw=2, solid_capstyle='round', ms=7)\n",
    "    configs = [(train, \"Precision\", \"#1B9CFC\", \"Train\", \"-\", \"o\"),\n",
    "               (test, \"Precision\", \"#1B9CFC\", \"Test\",  \"--\", \"o\"),\n",
    "               (train, \"Recall\", \"#FC427B\", \"Train\", \"-\", \"s\"),\n",
    "               (test, \"Recall\", \"#FC427B\", \"Test\", \"--\", \"s\")]\n",
    "    \n",
    "    for n, (data, col, color, which, ls, marker) in enumerate(configs):\n",
    "        ax.plot(data[col.lower()], color=color, ls=ls, marker=None, \n",
    "                label=\"{} ({})\".format(col, which), **base_style)\n",
    "        if n==1:\n",
    "            ax.fill_between(x, train[\"precision\"], test[\"precision\"], \n",
    "                    color=\"#1B9CFC\", alpha=0.2, label=\"Precision Gap\")\n",
    "        elif n==3:\n",
    "            ax.fill_between(x, train[\"recall\"], test[\"recall\"], \n",
    "                    color=\"#FC427B\", alpha=0.2, label=\"Recall Gap\")\n",
    "\n",
    "    ax.set_ylabel(\"Precision & Recall\", fontsize=13, fontweight=1000)\n",
    "    ax.set_xlabel(\"Number of Rules\", fontsize=13, fontweight=1000)\n",
    "    ax.legend(loc=\"best\", fontsize=12, framealpha=0, ncol=2, \n",
    "              columnspacing=0.5, handletextpad=0.8, \n",
    "              labelspacing=0.4, handlelength=1.5)\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "    ax.tick_params(axis='both', labelsize=13)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: class (\">50K\" vs \"<=50K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fetch_openml(\"adult\", version=2, as_frame=True).frame.drop(columns=[\"fnlwgt\", \"education-num\"])\n",
    "y = X.pop(\"class\").values\n",
    "y = np.where(y==\">50K\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical to numerical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = list(X.select_dtypes(include=[\"category\"]))\n",
    "for c in cat: X[c] = X[c].cat.add_categories('missing').fillna('missing')\n",
    "enc = OneHotEncoder(handle_unknown='ignore').fit(X[cat])\n",
    "columns = [\"{} ({})\".format(*n) \n",
    "           for c,v in zip(cat, enc.categories_) \n",
    "           for n in list(product([c],v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.DataFrame(enc.transform(X[cat]).toarray().astype(int), columns=columns)\n",
    "cat_X  = X.drop(columns=cat).merge(cat_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_train, Xt_test, yt_train, yt_test = tts(cat_X, y, test_size=0.3, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of rules\n",
    "- Extract the best path in tree that satisfies criteria.\n",
    "- If **`exclude`=True**, after each iteration training samples under previously selected leaf node (path) are excluded from the training set before determining the next rule. If **`exclude`=False**, it changes target to non-target i.e. 1 to 0 while keeping the sample size the same.\n",
    "- This approach stops when the evaluating metric is deemed satisfactory (`max_iter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TreeRuleMining.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: **`DecisionTreeClassifier`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = dict(max_depth=None,\n",
    "            max_features=Xt_train.shape[1], \n",
    "            random_state=0, \n",
    "            min_samples_leaf=0.01, \n",
    "            class_weight=\"balanced\")\n",
    "Tree1 = DecisionTreeClassifier(**kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = TreeRuleMining(Tree1, \n",
    "                        exclude=True, \n",
    "                        metric=\"recall\", \n",
    "                        max_iter=50).fit(Xt_train, yt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply selected rules on `X` and evaluate rule performance against `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train1 = model1.evaluate(Xt_train, yt_train, cumulative=True)\n",
    "eval_test1  = model1.evaluate(Xt_test , yt_test , cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of visualizations using **`plot_results`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_results(eval_train1, eval_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all selected rules and their subrules using **`print_rule`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_rules = 5\n",
    "for n in np.arange(n_rules)+1:\n",
    "    print(key:=f\"Rule_{n}\")\n",
    "    print_rule(model1.rules[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **`self.transform`** to convert rules into features array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.transform(Xt_train, n_rules).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary on `Xt_train` using **`print_stats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_pred_train = model1.transform(Xt_train, n_rules).sum(1)>0\n",
    "print_stats(yt_train, yt_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_pred_test = model1.transform(Xt_test, n_rules).sum(1)>0\n",
    "print_stats(yt_test, yt_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: **`RandomForestClassifier`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = dict(n_estimators=20,\n",
    "            max_depth=None,  \n",
    "            min_samples_leaf=0.01,\n",
    "            max_features=\"sqrt\", \n",
    "            random_state=0, \n",
    "            bootstrap=True)\n",
    "Tree2 = RandomForestClassifier(**kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = TreeRuleMining(Tree2, \n",
    "                        exclude=True, \n",
    "                        metric=\"recall\", \n",
    "                        max_iter=50).fit(Xt_train, yt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train2 = model2.evaluate(Xt_train, yt_train, cumulative=True)\n",
    "eval_test2  = model2.evaluate(Xt_test , yt_test , cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_results(eval_train2, eval_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: **`ExtraTreesClassifier`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = dict(n_estimators=20, \n",
    "            criterion=\"gini\",\n",
    "            max_depth=None,  \n",
    "            min_samples_leaf=0.01,\n",
    "            max_features=\"sqrt\", \n",
    "            random_state=0, \n",
    "            bootstrap=True, \n",
    "            monotonic_cst=[1]*Xt_train.shape[1])\n",
    "Tree3 = ExtraTreesClassifier(**kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = TreeRuleMining(Tree3, \n",
    "                        exclude=True, \n",
    "                        metric=\"recall\", \n",
    "                        max_iter=50).fit(Xt_train, yt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train3 = model3.evaluate(Xt_train, yt_train, cumulative=True)\n",
    "eval_test3  = model3.evaluate(Xt_test , yt_test , cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_results(eval_train3, eval_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
